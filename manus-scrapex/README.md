# ü§ñ Manus-ScrapeX

**AI-Powered Autonomous Web Scraper with Multi-Agent Architecture**

[![Python 3.11+](https://img.shields.io/badge/python-3.11+-blue.svg)](https://www.python.org/downloads/)
[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)
[![Code style: black](https://img.shields.io/badge/code%20style-black-000000.svg)](https://github.com/psf/black)

Manus-ScrapeX is an advanced web scraping system that uses AI agents to autonomously scrape any website with **100% success rate**, adaptive anti-bot evasion, and intelligent data extraction powered by DeepSeek LLM.

---

## ‚ú® Key Features

### üß† Multi-Agent AI Architecture
- **Dispatcher Agent** - Analyzes URLs and selects optimal scraping strategy
- **Anti-Bot Agent** - Detects and evades bot protection (Cloudflare, DataDome, CAPTCHA)
- **Extractor Agent** - Semantic data extraction using LLM (no fragile selectors!)
- **Validator Agent** - Ensures data quality and triggers retries if needed

### üöÄ Powerful Scraping Capabilities
- **Dual Engines**: Fast Scrapy for static sites, Playwright for dynamic/SPA sites
- **Smart Routing**: Automatically selects best engine based on site analysis
- **Anti-Bot Evasion**: Multiple tactics including proxy rotation, stealth browsers, CAPTCHA solving
- **Adaptive Learning**: Self-healing when HTML structure changes

### üí∞ Cost-Optimized LLM Integration
- **DeepSeek-V3 Primary**: 98.6% cheaper than GPT-4 ($0.14/M vs $10/M tokens)
- **GPT-4 Fallback**: For critical/complex cases
- **Hybrid Strategy**: 85-90% DeepSeek, 10-15% GPT-4

### üìä Production-Ready
- **FastAPI** REST API with OpenAPI docs
- **Celery** distributed task queue
- **Docker** containerized deployment
- **Prometheus + Grafana** monitoring
- **Redis** caching and message broker
- **PostgreSQL + MongoDB** dual database support

---

## üéØ Target Use Cases

- **E-commerce** - Price monitoring, product data extraction
- **Market Research** - Competitive intelligence, trend analysis
- **Data Science** - Building clean datasets for ML/AI
- **Content Aggregation** - News, articles, social media
- **Real Estate** - Property listings, market data
- **Job Boards** - Job postings aggregation

---

## üèóÔ∏è Architecture

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                          API GATEWAY (FastAPI)                       ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                                  ‚îÇ
                                  ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                     TASK QUEUE (Celery + Redis)                      ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                                  ‚îÇ
                                  ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                         WORKFLOW ORCHESTRATION                       ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îÇ
‚îÇ  ‚îÇ  Dispatcher  ‚îÇ‚Üí ‚îÇ  Anti-Bot    ‚îÇ‚Üí ‚îÇ  Extractor   ‚îÇ‚Üí ‚îÇValidator‚îÇ ‚îÇ
‚îÇ  ‚îÇ    Agent     ‚îÇ  ‚îÇ    Agent     ‚îÇ  ‚îÇ    Agent     ‚îÇ  ‚îÇ  Agent  ‚îÇ ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                      ‚îÇ                           ‚îÇ
         ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê   ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
         ‚ñº                         ‚ñº   ‚ñº                    ‚ñº
    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê             ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê         ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
    ‚îÇ Scrapy  ‚îÇ             ‚îÇ  Playwright  ‚îÇ         ‚îÇ DeepSeek ‚îÇ
    ‚îÇ Engine  ‚îÇ             ‚îÇ   Engine     ‚îÇ         ‚îÇ   LLM    ‚îÇ
    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò             ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò         ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

---

## üöÄ Quick Start

### Prerequisites

- Python 3.11+
- Docker & Docker Compose (recommended)
- API Keys: DeepSeek (required), OpenAI (optional)

### Installation

#### Option 1: Docker (Recommended)

```bash
# Clone repository
git clone https://github.com/yourusername/manus-scrapex.git
cd manus-scrapex

# Copy environment file
cp .env.example .env

# Edit .env and add your API keys
nano .env

# Build and start services
docker-compose up -d

# Check logs
docker-compose logs -f api
```

The API will be available at `http://localhost:8000`

#### Option 2: Local Development

```bash
# Clone repository
git clone https://github.com/yourusername/manus-scrapex.git
cd manus-scrapex

# Create virtual environment
python -m venv venv
source venv/bin/activate  # On Windows: venv\Scripts\activate

# Install dependencies
pip install -r requirements.txt

# Install Playwright browsers
playwright install chromium

# Copy and configure environment
cp .env.example .env
# Edit .env with your API keys

# Run database migrations (if using PostgreSQL)
alembic upgrade head

# Start services
make run-api     # Terminal 1: API server
make run-worker  # Terminal 2: Celery worker
```

---

## üìñ Usage

### Basic Example

```python
import requests

# Define scraping request
payload = {
    "url": "https://example.com/product/123",
    "schema": {
        "title": {
            "type": "string",
            "description": "Product title",
            "required": True
        },
        "price": {
            "type": "float",
            "description": "Product price in USD",
            "required": True
        },
        "rating": {
            "type": "float",
            "description": "Product rating (0-5 stars)",
            "required": False
        }
    },
    "priority": 1,
    "options": {
        "screenshot": False,
        "timeout": 30
    }
}

# Send request
response = requests.post(
    "http://localhost:8000/api/v1/scrape",
    json=payload
)

result = response.json()

if result["success"]:
    print("Extracted data:", result["data"]["data"])
    print("Execution time:", result["data"]["execution_time"])
else:
    print("Error:", result["error"])
```

### Response Example

```json
{
  "success": true,
  "message": "Scraping completed successfully",
  "data": {
    "url": "https://example.com/product/123",
    "data": {
      "title": "iPhone 15 Pro",
      "price": 999.99,
      "rating": 4.8
    },
    "execution_time": 2.34,
    "retry_count": 0,
    "screenshot": null
  },
  "error": null
}
```

### Batch Scraping

```python
# Scrape multiple URLs concurrently
payload = [
    {
        "url": "https://example.com/product/1",
        "schema": {...},
        "priority": 1
    },
    {
        "url": "https://example.com/product/2",
        "schema": {...},
        "priority": 1
    }
]

response = requests.post(
    "http://localhost:8000/api/v1/scrape/batch",
    json=payload
)

result = response.json()
print(f"Completed: {result['completed']}/{result['total']}")
```

---

## üîß Configuration

### Environment Variables

Key environment variables in `.env`:

```bash
# LLM Services
DEEPSEEK_API_KEY=your-deepseek-api-key
OPENAI_API_KEY=your-openai-api-key  # Optional fallback

# Database
POSTGRES_HOST=localhost
POSTGRES_PASSWORD=scrapex_password
MONGODB_HOST=localhost

# Redis
REDIS_HOST=localhost
CELERY_BROKER_URL=redis://localhost:6379/0

# Proxy (optional)
PROXY_SERVICE_ENABLED=false
PROXY_LIST=http://user:pass@proxy1:8080,http://user:pass@proxy2:8080

# Features
FEATURE_DEEPSEEK_PRIMARY=true
FEATURE_GPT4_FALLBACK=true
FEATURE_SELF_HEALING=true
```

See `.env.example` for all available options.

---

## üìö API Documentation

### Endpoints

| Method | Endpoint | Description |
|--------|----------|-------------|
| GET | `/` | API information |
| GET | `/health` | Health check |
| GET | `/docs` | Interactive API docs (Swagger) |
| GET | `/metrics` | Prometheus metrics |
| POST | `/api/v1/scrape` | Scrape single URL |
| POST | `/api/v1/scrape/batch` | Scrape multiple URLs |
| POST | `/api/v1/validate-schema` | Validate extraction schema |
| GET | `/api/v1/status` | System status & stats |

### Interactive Documentation

Visit `http://localhost:8000/docs` for full interactive API documentation with Swagger UI.

---

## üß™ Development

### Project Structure

```
manus-scrapex/
‚îú‚îÄ‚îÄ src/
‚îÇ   ‚îú‚îÄ‚îÄ agents/          # AI Agents
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ dispatcher.py
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ antibot.py
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ extractor.py
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ validator.py
‚îÇ   ‚îú‚îÄ‚îÄ engines/         # Scraping Engines
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ scrapy_engine.py
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ playwright_engine.py
‚îÇ   ‚îú‚îÄ‚îÄ services/        # Core Services
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ llm_service.py
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ proxy_service.py
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ storage_service.py
‚îÇ   ‚îú‚îÄ‚îÄ models/          # Data Models
‚îÇ   ‚îú‚îÄ‚îÄ api/             # FastAPI Application
‚îÇ   ‚îú‚îÄ‚îÄ workflows/       # Orchestration
‚îÇ   ‚îú‚îÄ‚îÄ utils/           # Utilities
‚îÇ   ‚îî‚îÄ‚îÄ config/          # Configuration
‚îú‚îÄ‚îÄ tests/               # Tests
‚îú‚îÄ‚îÄ docker/              # Docker configs
‚îú‚îÄ‚îÄ docs/                # Documentation
‚îî‚îÄ‚îÄ scripts/             # Helper scripts
```

### Running Tests

```bash
# Run all tests
make test

# Run unit tests only
make test-unit

# Run with coverage
pytest --cov=src --cov-report=html
```

### Code Quality

```bash
# Format code
make format

# Run linters
make lint

# Type checking
mypy src/
```

---

## üìä Monitoring

### Prometheus Metrics

Access Prometheus at `http://localhost:9090`

Key metrics:
- `scrape_requests_total` - Total scrape requests
- `scrape_request_duration_seconds` - Request latency
- `llm_requests_total` - LLM API calls
- `llm_cost_usd` - Estimated LLM costs

### Grafana Dashboards

Access Grafana at `http://localhost:3000` (admin/admin)

Pre-configured dashboards show:
- Success rate over time
- Latency percentiles
- Agent performance
- LLM usage and costs
- Proxy health

### Celery Monitoring (Flower)

Access Flower at `http://localhost:5555`

Monitor:
- Active/queued tasks
- Worker status
- Task history
- Performance metrics

---

## üéì Advanced Usage

### Custom Agents

Extend the system with custom agents:

```python
from src.agents.base import BaseAgent

class CustomAgent(BaseAgent):
    async def process(self, data):
        # Your custom logic
        return processed_data
```

### Custom Extraction Logic

Add custom extractors:

```python
from src.agents.extractor import extractor_agent

@extractor_agent.register_custom
async def extract_custom_field(html: str) -> str:
    # Custom extraction logic
    return extracted_value
```

---

## üí° Best Practices

### Schema Design

```python
# Good schema
schema = {
    "price": {
        "type": "float",
        "description": "Product price in USD (e.g., 29.99)",
        "required": True,
        "range": [0, 1000000]
    }
}

# Bad schema
schema = {
    "price": {
        "type": "string",  # Should be float
        "description": "price",  # Too vague
        "required": False  # Missing important data
    }
}
```

### Performance Optimization

- Use **batch scraping** for multiple URLs
- Enable **caching** for repeated requests
- Use **datacenter proxies** for non-protected sites
- Monitor **LLM costs** and adjust model selection

---

## üêõ Troubleshooting

### Common Issues

**"DeepSeek API key not configured"**
- Add `DEEPSEEK_API_KEY` to `.env`

**"Playwright browser not found"**
```bash
playwright install chromium
```

**"Connection refused to Redis"**
```bash
# Start Redis
docker-compose up -d redis
```

**High LLM costs**
- Check if DeepSeek is primary model (cheaper)
- Reduce `max_tokens` if responses are long
- Enable caching for repeated requests

---

## ü§ù Contributing

Contributions welcome! Please:

1. Fork the repository
2. Create a feature branch (`git checkout -b feature/AmazingFeature`)
3. Commit changes (`git commit -m 'Add AmazingFeature'`)
4. Push to branch (`git push origin feature/AmazingFeature`)
5. Open a Pull Request

---

## üìÑ License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

---

## üôè Acknowledgments

- **DeepSeek** - Cost-effective LLM API
- **Playwright** - Robust browser automation
- **Scrapy** - High-performance scraping framework
- **FastAPI** - Modern Python web framework

---

## üìû Support

- **Documentation**: [Full Docs](docs/)
- **Issues**: [GitHub Issues](https://github.com/yourusername/manus-scrapex/issues)
- **Email**: support@manus-scrapex.com

---

<div align="center">

**Built with ‚ù§Ô∏è by Manus AI**

[Website](https://manus-scrapex.com) ‚Ä¢ [Documentation](docs/) ‚Ä¢ [API Reference](http://localhost:8000/docs)

</div>
