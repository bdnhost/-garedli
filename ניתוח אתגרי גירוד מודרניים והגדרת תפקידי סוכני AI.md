# ניתוח אתגרי גירוד מודרניים והגדרת תפקידי סוכני AI

השאיפה ל-**100% הצלחה** בגירוד רשת מחייבת התמודדות עם אתגרים מודרניים שאינם ניתנים לפתרון באמצעות ספריות גירוד מסורתיות בלבד. הפתרון המוצע הוא מערך **סוכני AI אוטונומיים** (Multi-Agent System) שיפעלו בשיתוף פעולה.

## אתגרי הגירוד המרכזיים

| אתגר | תיאור | דרישת פתרון |
| :--- | :--- | :--- |
| **אתרים דינמיים (JavaScript)** | אתרים מודרניים רבים נבנים כ-SPA (Single Page Applications) וטוענים תוכן באמצעות JavaScript. גירוד מסורתי (Requests) יחזיר רק את קוד המקור הריק. | **רינדור מלא** של הדף באמצעות דפדפן חסר ראש (Headless Browser). |
| **מערכות Anti-Bot** | חסימות מתוחכמות כמו Cloudflare, DataDome, PerimeterX, המזהות בוטים באמצעות ניתוח טביעת אצבע של הדפדפן, קצב הבקשות, ואינטראקציות לא אנושיות. | **הסוואה אנושית** (Human Emulation), ניהול פרוקסי מתקדם, ושימוש ב-AI לפתרון אתגרים. |
| **שינויים במבנה האתר (Schema Drift)** | אתרים משנים את מבנה ה-HTML שלהם באופן תדיר, מה ששובר את ה-Selectors (XPath/CSS) של הסקרייפר. | **אדפטיביות** ושימוש ב-AI לזיהוי ויזואלי של נתונים (Visual Recognition) במקום הסתמכות על מבנה קשיח. |
| **איכות הנתונים ושלמותם** | הצורך לוודא שהנתונים שנגרדו שלמים, נקיים משגיאות, ועומדים בפורמט המבוקש. | **וולידציה אוטומטית** באמצעות AI, כולל השוואה בין הנתונים הגולמיים לתוצאה הסופית. |

## מערך סוכני ה-AI המוצע (The AI Scraper Agents)

הארכיטקטורה תתבסס על ארבעה סוכנים עיקריים, שכל אחד מהם מתמחה בתחום אחר:

### 1. סוכן ניהול ובחירת אסטרטגיה (The Dispatcher Agent)

*   **תפקיד:** הסוכן המרכזי המקבל את ה-URL ומחליט על אסטרטגיית הגירוד הראשונית.
*   **מודל קבלת החלטות:**
    1.  **בדיקה ראשונית:** שולח בקשת `HEAD` או `GET` פשוטה (Requests) לבדיקת קוד הסטטוס ונוכחות JavaScript (באמצעות ניתוח קוד המקור).
    2.  **בחירת כלי:**
        *   אם אין JavaScript (אתר סטטי): מפנה לליבת **Scrapy** + **Beautiful Soup**.
        *   אם יש JavaScript (אתר דינמי): מפנה לליבת **Playwright** (דפדפן חסר ראש).
    3.  **ניהול משאבים:** מקצה פרוקסי מתאים (ממאגר הפרוקסי) ומגדיר את ה-User-Agent.

### 2. סוכן עקיפת חסימות (The Anti-Bot Agent)

*   **תפקיד:** מתמחה בזיהוי ועקיפת אתגרי Anti-bot.
*   **פעולות:**
    1.  **זיהוי חסימה:** מנתח את תוכן הדף (או קוד הסטטוס) לאיתור הודעות חסימה (כגון "Checking your browser before accessing", CAPTCHA).
    2.  **הסוואה:** מפעיל מנגנוני "חיזוק" (Fortification) לדפדפן (שינוי טביעת אצבע, הוספת עוגיות).
    3.  **פתרון אתגרים:** משתמש במודל ראייה (Vision Model) לפתרון CAPTCHA או מנגנוני אימות ויזואליים אחרים.
    4.  **החלפת פרוקסי:** מורה לסוכן הדיספצ'ר להחליף כתובת IP במקרה של חסימה קשה.

### 3. סוכן ניתוח ומיצוי נתונים (The Extractor Agent)

*   **תפקיד:** אחראי על ניתוח ה-HTML שסופק על ידי כלי הגירוד ומיצוי הנתונים לפורמט מובנה (JSON/CSV).
*   **פעולות:**
    1.  **זיהוי סכמה:** משתמש ב-LLM (Large Language Model) כדי לזהות את מבנה הנתונים הרצוי (למשל, "כותרת", "מחיר", "תיאור") על בסיס ההקשר.
    2.  **מיצוי אדפטיבי:** במקום להסתמך על Selectors קשיחים, משתמש ב-AI כדי לזהות את האלמנטים הוויזואליים הרלוונטיים בדף (Visual Recognition) ומוציא את הטקסט.
    3.  **טיפול ב-Edge Cases:** מזהה חוסר עקביות בנתונים ומנסה לתקן או להשלים אותם (למשל, המרת מטבעות, ניקוי תווים לא רצויים).

### 4. סוכן וולידציה ובקרת איכות (The Validator Agent)

*   **תפקיד:** הבטחת איכות הנתונים ועמידה בדרישות ה-100%.
*   **פעולות:**
    1.  **בדיקת שלמות:** מוודא שכל השדות הנדרשים נגרדו.
    2.  **בדיקת עקביות:** משווה את הנתונים הממוצים לכללים לוגיים (למשל, מחיר חייב להיות מספר חיובי).
    3.  **דיווח כשל:** במקרה של כשל בגירוד או נתונים חסרים, מדווח לסוכן הדיספצ'ר ומציע אסטרטגיית גירוד חלופית (למשל, ניסיון חוזר עם פרוקסי אחר, או שימוש ב-API של צד שלישי).

השילוב של סוכנים אלו מאפשר גמישות, אדפטיביות, ויכולת למידה, שהם המפתח להשגת שיעור הצלחה גבוה ככל האפשר.
