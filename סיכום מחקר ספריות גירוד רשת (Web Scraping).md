# סיכום מחקר ספריות גירוד רשת (Web Scraping)

## ספריות וכלי גירוד מובילים (2025)

המחקר הראשוני זיהה ארבע קטגוריות עיקריות של כלי גירוד, כאשר הבחירה תלויה באופי האתר (סטטי מול דינמי) ובקנה המידה של הפרויקט.

| קטגוריה | כלים עיקריים | תיאור ושימוש מומלץ | יתרונות | חסרונות |
| :--- | :--- | :--- | :--- | :--- |
| **ספריות ניתוח (Parsing)** | Beautiful Soup (Python), lxml (Python) | אידיאלי לניתוח תוכן HTML/XML לאחר שהורד. מצוין לאתרים סטטיים או לניתוח תוצאות שהתקבלו מכלי אחר. | קל לשימוש, מהיר, צריכת משאבים נמוכה. | לא מטפל בהורדת תוכן, לא תומך ב-JavaScript. |
| **ספריות HTTP (הורדה)** | Requests (Python), Axios (Node.js) | משמש להורדת תוכן האתר (HTML גולמי) בצורה פשוטה וישירה. | פשוט, מהיר, קל לשילוב. | לא מטפל ב-JavaScript, לא מנתח את התוכן. |
| **Frameworks מלאים** | Scrapy (Python) | Framework אסינכרוני מלא המיועד לגירוד בקנה מידה גדול, כולל ניהול בקשות, עיבוד, ו-Pipelines. | ביצועים גבוהים, סקיילביליות, מובנה לגירוד מרובה. | עקומת למידה תלולה, דורש שילוב עם כלי אחר לטיפול ב-JavaScript. |
| **דפדפנים חסרי ראש (Headless Browsers)** | Playwright (Python/Node.js), Puppeteer (Node.js) | חיוני לגירוד אתרים דינמיים המשתמשים ב-JavaScript (SPA). מדמה אינטראקציה אנושית מלאה (קליקים, גלילה). | מטפל ב-JavaScript, עוקף אתגרי Anti-bot בסיסיים, API מודרני (Playwright). | איטי, צורך משאבים גבוה, ניתן לחסימה על ידי מערכות Anti-bot מתקדמות. |

## אתגרי גירוד מודרניים (השאיפה ל-100%)

השאיפה ל-100% הצלחה מחייבת התמודדות עם אתגרי Anti-bot מודרניים:

1.  **זיהוי דפדפן חסר ראש (Headless Detection):** אתרים מזהים חתימות של דפדפנים חסרי ראש (כמו Playwright/Puppeteer) וחוסמים אותם. הפתרון דורש "חיזוק" (Fortification) של הדפדפן כדי לחקות טביעת אצבע (Fingerprint) של משתמש אמיתי.
2.  **חסימת כתובות IP ו-Rate Limiting:** דורש שימוש במערך פרוקסי (Proxy) איכותי ומסתובב (Rotating), עדיף Residential או Mobile, כדי להבטיח כתובות IP נקיות.
3.  **אתגרי JavaScript (CAPTCHA, Cloudflare):** דורש שימוש בפתרונות צד שלישי (כמו ZenRows, ScrapingBee) או שילוב של סוכני AI שיודעים לפתור אתגרים אלו (כגון זיהוי תמונות או אינטראקציה מורכבת).

## המלצה ראשונית לארכיטקטורה

הארכיטקטורה המוצעת תהיה **היברידית** ותשלב את היתרונות של כלים שונים, בהתאם למשימה:

*   **ליבה:** **Scrapy** (לסקיילביליות וניהול תורים).
*   **טיפול ב-JavaScript:** **Playwright** (כמנוע רינדור חיצוני ל-Scrapy).
*   **ניתוח:** **Beautiful Soup** (לניתוח מהיר ומדויק של התוכן המרונדר).
*   **AI Agent Layer:** מערך סוכני AI שיחליט באיזה כלי להשתמש עבור כל URL, ינהל את מערך הפרוקסי, וינסה לעקוף חסימות באופן אוטונומי.

שלב 2 יתמקד בניתוח מעמיק יותר של יכולות ה-AI והסוכנים הנדרשים כדי להשיג את יעד ה-100%.
